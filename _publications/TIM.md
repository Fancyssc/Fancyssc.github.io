---
title: "TIM: an efficient temporal interaction module for spiking transformer"
collection: publications
category: conferences
permalink: /publication/TIM
excerpt: 'The first Spiking Transformer enhancement method from the perspective of temporal enhancement."
date: 2024-01-01
venue: 'IJCAI 2024'
citation: "Shen, S., Zhao, D., Shen, G., & Zeng, Y. (2024). TIM: an efficient temporal interaction module for spiking transformer."
paperurl: 'https://www.ijcai.org/proceedings/2024/0347.pdf'
---
**Abstract:** 

Spiking Neural Networks (SNNs), as the third generation of neural networks, have gained prominence for their biological plausibility and computational efficiency, especially in processing diverse datasets. The integration of attention mechanisms, inspired by advancements in neural network architectures, has led to the development of Spiking Transformers. These have shown promise in enhancing SNNs' capabilities, particularly in the realms of both static and neuromorphic datasets. Despite their progress, a discernible gap exists in these systems, specifically in the Spiking Self Attention (SSA) mechanism's effectiveness in leveraging the temporal processing potential of SNNs. To address this, we introduce the Temporal Interaction Module (TIM), a novel, convolution-based enhancement designed to augment the temporal data processing abilities within SNN architectures. TIM's integration into existing SNN frameworks is seamless and efficient, requiring minimal additional parameters while significantly boosting their temporal information handling capabilities. Through rigorous experimentation, TIM has demonstrated its effectiveness in exploiting temporal information, leading to state-of-the-art performance across various neuromorphic datasets.  


**Code:** [here](https://github.com/Fancyssc/TIM)

**Bibtex:**

```angular2html
@article{shen2024tim,
  title={TIM: an efficient temporal interaction module for spiking transformer},
  author={Shen, Sicheng and Zhao, Dongcheng and Shen, Guobin and Zeng, Yi},
  journal={arXiv preprint arXiv:2401.11687},
  year={2024}
}
```
